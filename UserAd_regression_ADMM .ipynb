{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Social_Network_Ads.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual one-hot encoding\n",
    "df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n",
    "\n",
    "# features used for predicting and the actual target variable\n",
    "X = df[['Gender', 'Age', 'EstimatedSalary']]\n",
    "y = df['Purchased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise the dataset - for better convergence in admm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation for regression - manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss function is defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function to be used in the ADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(X, y, beta, lambda_):\n",
    "    m = len(y)\n",
    "    predictions = sigmoid(X @ beta)\n",
    "    # the following is just a loss function formula\n",
    "    loss = - (1 / m) * (y @ np.log(predictions + 1e-9) + (1 - y) @ np.log(1 - predictions + 1e-9)) + (lambda_ / 2) * np.sum(beta ** 2)\n",
    "    # 1e-9 is added to the prediction to avoid log(0) numerical errors\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(a, b):\n",
    "    \"\"\"Computes the dot product of two vectors.\"\"\"\n",
    "    return sum(x * y for x, y in zip(a, b))\n",
    "\n",
    "def matrix_vector_mult(matrix, vector):\n",
    "    \"\"\"Multiplies a matrix with a vector.\"\"\"\n",
    "    return [dot_product(row, vector) for row in matrix]\n",
    "\n",
    "def vector_add(a, b, scale=1.0):\n",
    "    \"\"\"Adds two vectors element-wise with an optional scaling factor.\"\"\"\n",
    "    return [x + scale * y for x, y in zip(a, b)]\n",
    "\n",
    "def vector_subtract(a, b):\n",
    "    \"\"\"Subtracts vector b from vector a.\"\"\"\n",
    "    return [x - y for x, y in zip(a, b)]\n",
    "\n",
    "def scalar_vector_mult(scalar, vector):\n",
    "    \"\"\"Multiplies each element in a vector by a scalar.\"\"\"\n",
    "    return [scalar * x for x in vector]\n",
    "\n",
    "def l2_norm(vector):\n",
    "    \"\"\"Calculates the L2 norm of a vector.\"\"\"\n",
    "    return math.sqrt(sum(x ** 2 for x in vector))\n",
    "\n",
    "def logistic_loss(X, y, beta, lambda_):\n",
    "    \"\"\"Calculates logistic loss with L2 regularization.\"\"\"\n",
    "    m = len(y)\n",
    "    total_loss = 0\n",
    "    for i in range(m):\n",
    "        predicted = sigmoid(dot_product(X[i], beta))\n",
    "        total_loss += -y[i] * math.log(predicted) - (1 - y[i]) * math.log(1 - predicted)\n",
    "    regularization = (lambda_ / 2) * dot_product(beta, beta)\n",
    "    return (total_loss / m) + regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADMM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admm_logistic_regression(X, y, lambda_=1.0, rho=1.0, max_iter=1000, tol=1e-6):\n",
    "\n",
    "    m = len(X)\n",
    "    n = len(X[0])     \n",
    "\n",
    "\n",
    "    beta = [0.01] * n\n",
    "    z = [0] * n\n",
    "    u = [0] * n\n",
    "\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        gradient = [0] * n\n",
    "\n",
    "        for i in range(m):\n",
    "            error = sigmoid(dot_product(X[i], beta)) - y[i]\n",
    "            \n",
    "            for j in range(n):\n",
    "                gradient[j] += X[i][j] * error\n",
    "        \n",
    "        for j in range(n):\n",
    "            gradient[j] = (gradient[j] / m) + lambda_ * beta[j] + rho * (beta[j] - z[j] + u[j])\n",
    "\n",
    "        beta_update = [(gradient[j]) / (rho + lambda_ + m) for j in range(n)]\n",
    "        beta = vector_subtract(beta, beta_update)\n",
    "\n",
    "\n",
    "        z_old = z.copy()\n",
    "        z = vector_add(beta, u)\n",
    "\n",
    "\n",
    "        u = vector_add(u, vector_subtract(beta, z))\n",
    "\n",
    "\n",
    "        loss = logistic_loss(X, y, beta, lambda_)\n",
    "        predictions = [sigmoid(dot_product(X[i], beta)) >= 0.5 for i in range(m)]\n",
    "        accuracy = sum(pred == actual for pred, actual in zip(predictions, y)) / m\n",
    "\n",
    "        \n",
    "        primal_residual = l2_norm(vector_subtract(beta, z))\n",
    "        dual_residual = l2_norm(scalar_vector_mult(-rho, vector_subtract(z, z_old)))\n",
    "        if primal_residual < tol and dual_residual < tol:\n",
    "            print(f'The model converged after {iteration + 1} epochs.')\n",
    "\n",
    "            print(f'Final accuracy: {accuracy * 100:.2f}%')\n",
    "            return beta, accuracy  \n",
    "\n",
    "    print('The model did not converge within the maximum number of epochs.')\n",
    "    return beta, None  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model converged after 207 epochs.\n",
      "Final accuracy: 80.62%\n",
      "Optimized set of weights: [np.float64(0.0014701764530638435), np.float64(0.027304884057911625), np.float64(0.017679981460369917)]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.tolist() if not isinstance(X_train, list) else X_train\n",
    "y_train = y_train.tolist() if not isinstance(y_train, list) else y_train\n",
    "\n",
    "\n",
    "beta_admm, acc_val = admm_logistic_regression(X_train, y_train, 10.0, 1.0)\n",
    "print('Optimized set of weights:', beta_admm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
